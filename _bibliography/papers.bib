---
---

@string{aps = {American Physical Society,}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Turab publications (al-folio / jekyll-scholar friendly)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{kumar2022forgedcharacterdatasets,
  bibtex_show = {true},
  abbr        = {IJAIA},
  title       = {Forged Character Detection Datasets: Passports, Driving Licences and Visa Stickers},
  author      = {Kumar, Teerath and Turab, Muhammad and Talpur, Shahnawaz and Brennan, Rob and Bendechache, Malika},
  abstract    = {Forged documents specifically passport, driving licence and VISA stickers are used for fraud purposes including robbery, theft and many more. So detecting forged characters from documents is a significantly important and challenging task in digital forensic imaging. Forged characters detection has two big challenges. First challenge is, data for forged characters detection is extremely difficult to get due to several reasons including limited access of data, unlabeled data or work is done on private data. Second challenge is, deep learning (DL) algorithms require labeled data, which poses a further challenge as getting labeled is tedious, time-consuming, expensive and requires domain expertise. To end these issues, in this paper we propose a novel algorithm, which generates the three datasets namely forged characters detection for passport (FCD-P), forged characters detection for driving licence (FCD-D) and forged characters detection for VISA stickers (FCD-V). To the best of our knowledge, we are the first to release these datasets. The proposed algorithm starts by reading plain document images, simulates forging simulation tasks on five different countries' passports, driving licences and VISA stickers. Then it keeps the bounding boxes as a track of the forged characters as a labeling process. Furthermore, considering the real world scenario, we performed the selected data augmentation accordingly. Regarding the stats of datasets, each dataset consists of 15000 images having size of 950 x 550 of each. For further research purpose we release our algorithm code 1 and, datasets ie FCD-P 2, FCD-D 3 and FCD-V 4.},
  journal     = {International Journal of Artificial Intelligence \& Applications},
  volume      = {13},
  number      = {2},
  pages       = {21--35},
  year        = {2022},
  month       = mar,
  doi         = {10.5121/ijaia.2022.13202},
  url         = {https://doi.org/10.5121/ijaia.2022.13202},
  google_scholar_id = {u5HHmVD_uO8C},
  dimensions        = {true},
}

@article{turab2022multifeatureaudio,
  bibtex_show = {true},
  abbr        = {IJAIA},
  title       = {Investigating Multi-Feature Selection and Ensembling for Audio Classification},
  author      = {Turab, Muhammad and Kumar, Teerath and Bendechache, Malika and Saber, Takfarinas},
  abstract    = {Deep Learning (DL) algorithms have shown impressive performance in diverse domains. Among them, audio has attracted many researchers over the last couple of decades due to some interesting patterns--particularly in classification of audio data. For better performance of audio classification, feature selection and combination play a key role as they have the potential to make or break the performance of any DL model. To investigate this role, we conduct an extensive evaluation of the performance of several cutting-edge DL models (i.e., Convolutional Neural Network, EfficientNet, MobileNet, Supper Vector Machine and Multi-Perceptron) with various state-of-the-art audio features (i.e., Mel Spectrogram, Mel Frequency Cepstral Coefficients, and Zero Crossing Rate) either independently or as a combination (i.e., through ensembling) on three different datasets (i.e., Free Spoken Digits Dataset, Audio Urdu Digits Dataset, and Audio Gujarati Digits Dataset). Overall, results suggest feature selection depends on both the dataset and the model. However, feature combinations should be restricted to the only features that already achieve good performances when used individually (i.e., mostly Mel Spectrogram, Mel Frequency Cepstral Coefficients). Such feature combination/ensembling enabled us to outperform the previous state-of-the-art results irrespective of our choice of DL model.},
  journal     = {International Journal of Artificial Intelligence \& Applications},
  volume      = {13},
  number      = {3},
  pages       = {69--84},
  year        = {2022},
  month       = may,
  doi         = {10.5121/ijaia.2022.13306},
  url         = {https://doi.org/10.5121/ijaia.2022.13306},
  google_scholar_id = {d1gkVwhDpl0C},
  selected = {true},
  dimensions        = {true},
}

@inproceedings{kumar2022detectiondatasets,
  bibtex_show = {true},
  abbr        = {AISCA},
  title       = {Detection Datasets: Forged Characters for Passport and Driving Licence},
  author      = {Kumar, Teerath and Turab, Muhammad and Talpur, Shahnawaz and Brennan, Rob and Bendechache, Malika},
  abstract    = {Forged characters detection from personal documents including a passport or a driving licence is an extremely important and challenging task in digital image forensics, as forged information on personal documents can be used for fraud purposes including theft, robbery etc. For any detection task ie forged character detection, deep learning models are data hungry and getting the forged characters dataset for personal documents is very difficult due to various reasons, including information privacy, unlabeled data or existing work is evaluated on private datasets with limited access and getting data labelled is another big challenge. To address these issues, we propose a new algorithm that generates two new datasets named forged characters detection on passport (FCD-P) and forged characters detection on driving licence (FCD-D). To the best of our knowledge, we are the first to release these datasets. The proposed algorithm first reads the plain image, then performs forging tasks ie randomly changes the position of the random character or randomly adds little noise. At the same time, the algorithm also records the bounding boxes of the forged characters. To meet real world situations, we perform multiple data augmentation on cards very carefully. Overall, each dataset consists of 15000 images, each image with size of 950 x 550. Our algorithm code, FCD-P and FCD-D are publicly available.},
  booktitle   = {6th International Conference on Artificial Intelligence, Soft Computing and Applications (AISCA 2022)},
  pages       = {45--54},
  year        = {2022},
  doi         = {10.5121/csit.2022.120204},
  url         = {https://doi.org/10.5121/csit.2022.120204},
  google_scholar_id = {2osOgNQ5qMEC},
  dimensions        = {true},
}

@inproceedings{khan2022ddr,
  bibtex_show = {true},
  abbr        = {ICETECC},
  title       = {Data Dimension Reduction Makes ML Algorithms Efficient},
  author      = {Khan, Wisal and Turab, Muhammad and Ahmad, Waqas and Ahmad, Syed Hasnat and Kumar, Kelash and Luo, Bin},
  abstract    = {Data dimension reduction (DDR) is all about mapping data from high dimensions to low dimensions, various techniques of DDR are being used for image dimension reduction like Random Projections, Principal Component Analysis (PCA), the Variance approach, LSA-Transform, the Combined and Direct approaches, and the New Random Approach. Auto-encoders (AE) are used to learn end-to-end mapping. In this paper, we demonstrate that pre-processing not only speeds up the algorithms but also improves accuracy in both supervised and unsupervised learning. In pre-processing of DDR, first PCA based DDR is used for supervised learning, then we explore AE based DDR for unsupervised learning. In PCA based DDR, we first compare supervised learning algorithms accuracy and time before and after applying PCA. Similarly, in AE based DDR, we compare unsupervised learning algorithm accuracy and time before and after AE representation learning. Supervised learning algorithms including support-vector machines (SVM), Decision Tree with GINI index, Decision Tree with entropy and Stochastic Gradient Descent classifier (SGDC) and unsupervised learning algorithm including K-means clustering, are used for classification purpose. We used two datasets MNIST and FashionMNIST Our experiment shows that there is massive improvement in accuracy and time reduction after pre-processing in both supervised and unsupervised learning.},
  booktitle   = {2022 International Conference on Emerging Technologies in Electronics, Computing and Communication (ICETECC)},
  year        = {2022},
  doi         = {10.1109/ICETECC56662.2022.10069527},
  url         = {https://doi.org/10.1109/ICETECC56662.2022.10069527},
  organization= {IEEE},
  google_scholar_id = {qjMakFHDy7sC},
  dimensions        = {true},
}

@inproceedings{sarwar2022audioaid,
  bibtex_show = {true},
  abbr        = {ICETECC},
  title       = {Advanced Audio Aid for Blind People},
  author      = {Sarwar, Savera and Turab, Muhammad and Channa, Danish and Chandio, Aisha and Sohu, M. Uzair and Kumar, Vikram},
  abstract    = {One of the most important senses in human life is vision, without it one’s life is totally filled with darkness. According to WHO globally millions of people are visually impaired estimated there are 285 million, of whom some millions are blind. Unfortunately, there are around 2.4 million people are blind in our beloved country Pakistan. Human are a crucial part of society and the blind community is a main part of society. The technologies are grown so far to make the life of humans easier more comfortable and more reliable for. However, this disability of the blind community would reduce their chance of using such innovative products. Therefore, the visually impaired community believe that they are burden to other societies and they do not capture in normal activities separates the blind people from society and because of this believe did not participate in the normally tasks of society. The visual impair people mainly face most of the problems in this real-time The aim of this work is to turn the real time world into an audio world by telling blind person about the objects in their way and can read printed text. This will enable blind persons to identify the things and read the text without any external help just by using the object detection and reading system in real time. Objective of this work: i) Object detection ii) Read printed text, using state-of-the-art (SOTA) technology.},
  booktitle   = {2022 International Conference on Emerging Technologies in Electronics, Computing and Communication (ICETECC)},
  year        = {2022},
  doi         = {10.1109/ICETECC56662.2022.10069052},
  url         = {https://doi.org/10.1109/ICETECC56662.2022.10069052},
  organization= {IEEE},
  google_scholar_id = {UeHWp8X0CEIC},
  dimensions        = {true},
}

@inproceedings{irfan2023gotogether,
  bibtex_show = {true},
  abbr        = {IMTIC},
  title       = {Go Together: Bridging the Gap between Learners and Teachers},
  author      = {Irfan, Asim and Nawaz, Atif and Turab, Muhammad and Azeem, Muhmmad and Adnan, Mashal and Mehmood, Ahsan and Ahmed, Sarfaraz and Ashraf, Adnan},
  booktitle   = {2023 7th International Multi-Topic ICT Conference (IMTIC)},
  pages       = {1--7},
  year        = {2023},
  doi         = {10.1109/IMTIC58887.2023.10178623},
  url         = {https://doi.org/10.1109/IMTIC58887.2023.10178623},
  organization= {IEEE},
  google_scholar_id = {W7OEmFMy1HYC},
}

@article{turab2023digitaltwinsmetaverse,
  bibtex_show = {true},
  abbr        = {BioMedInf},
  title       = {A Comprehensive Survey of Digital Twins in Healthcare in the Era of Metaverse},
  author      = {Turab, Muhammad and Jamil, Sonain},
  abstract    = {Digital twins (DTs) are becoming increasingly popular in various industries, and their potential for healthcare in the metaverse continues to attract attention. The metaverse is a virtual world where individuals interact with digital replicas of themselves and the environment. This paper focuses on personalized and precise medicine and examines the current application of DTs in healthcare within the metaverse. Healthcare practitioners may use immersive virtual worlds to replicate medical scenarios, improve teaching experiences, and provide personalized care to patients. However, the integration of DTs in the metaverse poses technical, regulatory, and ethical challenges that need to be addressed, including data privacy, standards, and accessibility. Through this examination, we aim to provide insights into the transformative potential of DTs in healthcare within the metaverse and encourage further research and development in this exciting domain.},
  journal     = {BioMedInformatics},
  volume      = {3},
  number      = {3},
  pages       = {563--584},
  year        = {2023},
  doi         = {10.3390/biomedinformatics3030039},
  url         = {https://doi.org/10.3390/biomedinformatics3030039},
  publisher   = {MDPI},
  selected = {true},
  google_scholar_id = {YsMSGLbcyi4C},
  dimensions        = {true},
}

@article{kumar2023audrandaug,
  bibtex_show   = {true},
  abbr          = {arXiv},
  title         = {AudRandAug: Random Image Augmentations for Audio Classification},
  author        = {Kumar, Teerath and Turab, Muhammad and Mileo, Alessandra and Bendechache, Malika and Saber, Takfarinas},
  abstract      = {Data augmentation has proven to be effective in training neural networks. Recently, a method called RandAug was proposed, randomly selecting data augmentation techniques from a predefined search space. RandAug has demonstrated significant performance improvements for image-related tasks while imposing minimal computational overhead. However, no prior research has explored the application of RandAug specifically for audio data augmentation, which converts audio into an image-like pattern. To address this gap, we introduce AudRandAug, an adaptation of RandAug for audio data. AudRandAug selects data augmentation policies from a dedicated audio search space. To evaluate the effectiveness of AudRandAug, we conducted experiments using various models and datasets. Our findings indicate that AudRandAug outperforms other existing data augmentation methods regarding accuracy performance.},
  journal       = {arXiv preprint arXiv:2309.04762},
  year          = {2023},
  doi           = {10.48550/arXiv.2309.04762},
  url           = {https://arxiv.org/abs/2309.04762},
  archivePrefix = {arXiv},
  eprint        = {2309.04762},
  google_scholar_id = {WF5omc3nYNoC},
  selected = {true},
  dimensions        = {true},
}

@article{memon2023alzheimerensemble,
  bibtex_show = {true},
  abbr        = {MURJET},
  title       = {An Ensemble of CNN Architectures for Early Detection of Alzheimer's Disease Using Brain MRI},
  author      = {Memon, Zainab and Turab, Muhammad and Narejo, Sanam and Korejo, Muhammad Tahir},
  abstract    = {Early detection of Alzheimer's disease (AD) has proven to be helpful and effective in preventing the disease. If the risks and symptoms of AD are detected earlier, then it seems rather promising that the death ratio of AD might decrease as it can help a lot of patients get treated before it's too late. Our study demonstrates promising results, achieving a remarkable accuracy of 96.52% through the utilization of the EfficientNetB2 and EfficientNetB3 models. By leveraging transfer learning, we leverage pre-trained models' knowledge to optimize the learning process, while ensemble learning further improves performance by aggregating predictions from multiple models. The integration of these methodologies provides an effective and efficient means of detecting Alzheimer's Disease at an early stage, thereby offering potential benefits to patients, caregivers, and healthcare providers alike. These findings pave the way for improved diagnostic tools and contribute to the advancement of AD research and patient care.},
  journal     = {Mehran University Research Journal of Engineering \& Technology},
  volume      = {42},
  number      = {4},
  pages       = {140--147},
  year        = {2023},
  doi         = {10.22581/muet1982.2304.2767},
  url         = {https://doi.org/10.22581/muet1982.2304.2767},
  google_scholar_id = {_FxGoFyzp5QC},
  dimensions        = {true},
}

@article{kumar2023imagedataaugmentation,
  bibtex_show   = {true},
  abbr          = {arXiv},
  title         = {Image Data Augmentation Approaches: A Comprehensive Survey and Future directions},
  author        = {Kumar, Teerath and Mileo, Alessandra and Brennan, Rob and Bendechache, Malika},
  journal       = {arXiv preprint arXiv:2301.02830},
  year          = {2023},
  doi           = {10.48550/arXiv.2301.02830},
  url           = {https://arxiv.org/abs/2301.02830},
  archivePrefix = {arXiv},
  eprint        = {2301.02830},
  google_scholar_id = {Wp0gIr-vW9MC},
  dimensions        = {true},
}

@article{turab2025lowillumination,
  bibtex_show   = {true},
  abbr          = {arXiv},
  title         = {A Comprehensive Survey on Image Signal Processing Approaches for Low-Illumination Image Enhancement},
  author        = {Turab, Muhammad},
  abstract      = {The usage of digital content (photos and videos) in a variety of applications has increased due to the popularity of multimedia devices. These uses include advertising campaigns, educational resources, and social networking platforms. There is an increasing need for high-quality graphic information as people become more visually focused. However, captured images frequently have poor visibility and a high amount of noise due to the limitations of image-capturing devices and lighting conditions. Improving the visual quality of images taken in low illumination is the aim of low-illumination image enhancement. This problem is addressed by traditional image enhancement techniques, which alter noise, brightness, and contrast. Deep learning-based methods, however, have dominated recently made advances in this area. These methods have effectively reduced noise while preserving important information, showing promising results in the improvement of low-illumination images. An extensive summary of image signal processing methods for enhancing low-illumination images is provided in this paper. Three categories are classified in the review for approaches: hybrid techniques, deep learning-based methods, and traditional approaches. Conventional techniques include denoising, automated white balancing, and noise reduction. Convolutional neural networks (CNNs) are used in deep learningbased techniques to recognize and extract characteristics from low-light images. To get better results, hybrid approaches combine deep learning-based methodologies with more conventional methods. The review also discusses the advantages and limitations of each approach and provides insights into future research directions in this field.},
  journal       = {arXiv preprint arXiv:2502.05995},
  year          = {2025},
  doi           = {10.48550/arXiv.2502.05995},
  url           = {https://arxiv.org/abs/2502.05995},
  archivePrefix = {arXiv},
  eprint        = {2502.05995},
  google_scholar_id = {qxL8FJ1GzNcC},
  dimensions        = {true},
}

@inproceedings{jamil2025ishihara,
  bibtex_show = {true},
  abbr        = {IPAS},
  title       = {Optimizing Color Vision Accuracy in Ishihara Tests through Spectral Illumination Optimization},
  author      = {Jamil, Sonain and Turab, Muhammad},
  abstract    = {In order to design optimal illumination that minimizes or maximizes differences between two sample points, we utilized particle swarm optimization (PSO), a popular algorithm for finding optimal solutions. In this study, we explored the application of PSO to minimize or maximize cost functions such as ∆E, ∆RGB, and Michelson contrast, and to output optimal intensities for designing the emission spectra of the illumination. This approach proved to be more convenient than altering the spectral properties of the object or the observer. The experiment involved capturing sample data from the Ishihara Color Blindness test book using Specim V10E, and measuring the emission spectra of ten LEDs from Ledmotive’s spectrally tunable LED light source using the Hamamatsu PMA-12 spectrometer. Subsequently, we pre-processed the captured data and applied PSO to obtain optimal intensities for the ten LEDs. The resulting optimized intensities were then used to compute the spectrally optimal emission spectrum. To assess the performance of these emission spectra, we illuminated the samples with the optimized spectra using the spectrally tunable LED light source. From the illuminated samples, we concluded that for the minimum cost function, the red number on the sample was not distinguishable from the green background, whereas for the maximum cost function, the red number was distinctive.},
  booktitle   = {2025 IEEE 6th International Conference on Image Processing, Applications and Systems (IPAS)},
  pages       = {1--6},
  year        = {2025},
  doi         = {10.1109/IPAS63548.2025.10924513},
  url         = {https://doi.org/10.1109/IPAS63548.2025.10924513},
  organization= {IEEE},
  google_scholar_id = {4DMP91E08xMC},
  dimensions        = {true},
}

@inproceedings{turab2026dancestyle,
  bibtex_show = {true},
  abbr        = {ACIVS},
  title       = {Dance Style Recognition Using Laban Movement Analysis},
  author      = {Turab, Muhammad and Colantoni, Philippe and Muselet, Damien and Tr{\'e}meau, Alain},
  abstract    = {The growing interest in automated movement analysis has presented new challenges in recognition of complex human activities including dance. This study focuses on dance style recognition using features extracted using Laban Movement Analysis. Previous studies for dance style recognition often focus on cross-frame movement analysis, which limits the ability to capture temporal context and dynamic transitions between movements. This gap highlights the need for a method that can add temporal context to LMA features. For this, we introduce a novel pipeline which combines 3D pose estimation, 3D human mesh reconstruction, and floor aware body modeling to effectively extract LMA features. To address the temporal limitation, we propose a sliding window approach that captures movement evolution across time in features. These features are then used to train various machine learning methods for classification, and their explainability using explainable AI methods to evaluate the contribution of each feature to classification performance. Our proposed method achieves a highest classification accuracy of 99.18% which shows that the addition of temporal context significantly improves dance style recognition performance.},
  booktitle   = {Advanced Concepts for Intelligent Vision Systems: 22nd International Conference, ACIVS 2025, Tokyo, Japan, July 28--30, 2025, Proceedings},
  pages       = {564--575},
  year        = {2026},
  doi         = {10.1007/978-3-032-07343-3_45},
  url         = {https://doi.org/10.1007/978-3-032-07343-3_45},
  publisher   = {Springer},
  google_scholar_id = {mVmsd5A6BfQC},
  selected = {true},
  dimensions        = {true},
}

@inproceedings{turab2025danceemotion,
  bibtex_show = {true},
  abbr        = {CAIP},
  title       = {Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis},
  author      = {Turab, Muhammad and Colantoni, Philippe and Muselet, Damien and Tr{\'e}meau, Alain},
  abstract    = {This paper presents a novel framework for emotion recognition in contemporary dance by improving existing Laban Movement Analysis (LMA) feature descriptors and introducing robust, novel descriptors that capture both quantitative and qualitative aspects of the movement. Our approach extracts expressive characteristics from 3D keypoints data of professional dancers performing contemporary dance under various emotional states, and train multiple classifiers, including Random Forests and Support Vector Machines. Additionally, provide in-depth explanation of features and their impact on model predictions using explainable machine learning methods. Overall, our study improves emotion recognition in contemporary dance and offers promising applications in performance analysis, dance training, and human–computer interaction with highest accuracy of 96.85%.},
  booktitle   = {Computer Analysis of Images and Patterns: 21st International Conference, CAIP 2025, Las Palmas de Gran Canaria, Spain, September 22--25, 2025, Proceedings},
  pages       = {317--328},
  year        = {2025},
  doi         = {10.1007/978-3-032-05060-1_27},
  url         = {https://doi.org/10.1007/978-3-032-05060-1_27},
  publisher   = {Springer},
  google_scholar_id = {9ZlFYXVOiuMC},
  selected = {true},
  dimensions        = {true},
}

@inproceedings{tremeau2026performingarts,
  bibtex_show = {true},
  abbr        = {SIPCOV},
  title       = {Movement Analysis in Performing Arts from Human Body Pose Estimation},
  author      = {Tr{\'e}meau, Alain and Turab, Muhammad and Muselet, Damien and Colantoni, Philippe},
  abstract    = {In this keynote paper we discuss how to automatically analyze the kinematics of human body movements in the context of dance analysis. We also propose new movement analysis handcrafted features based on Laban Movement Analysis (LMA) and 3D pose estimation of dancers. To evaluate the most significant body parts in kinematics analysis of human body movements, we computed the SHAP value of each body part. The preliminary tests and experiments we carried out using video sequences from AIST++ dataset demonstrate the relevance of these new features based either on short or long periods of time. We also demonstrate that the relevance of the most significant features depends on the specificities of the dance videos dataset to process. To improve the accuracy of handcraft features for dance style classification and their robustness to dataset specificities, we suggest to use domain adaptation techniques, such as feature-based, instance-based or parameter-based techniques.},
  booktitle   = {Advances on Signal Processing and Computer Vision: Second International Conference, SIPCOV 2025, Silchar, India, August 8--9, 2025, Proceedings},
  pages       = {331--345},
  year        = {2026},
  doi         = {10.1007/978-3-032-15809-3_26},
  url         = {https://doi.org/10.1007/978-3-032-15809-3_26},
  publisher   = {Springer},
  google_scholar_id = {qUcmZB5y_30C},
  dimensions        = {true},
}

@article{vavekanand2026edgeawaregan,
  bibtex_show = {true},
  abbr        = {AIL},
  title       = {A Stable Edge-Aware GAN Approach for Data Augmentation and Privacy-Preserving High-Fidelity CT Synthesis},
  author      = {Vavekanand, Raja and Turab, Muhammad and Kumar, Teerath},
  abstract    = {Generative adversarial networks (GANs) have shown remarkable potential in medical image synthesis but face persistent challenges in achieving diagnostic-grade quality, particularly in preserving anatomical edges and avoiding mode collapse. To address these limitations, we present a stable edge-aware GAN architecture featuring two key innovations: dynamically learnable bilateral kernels that adaptively enhance structural gradients during training, and a layered generator with interpolated skip connections to maintain spatial coherence. Our methodology leverages adversarial training with Wasserstein regularization on the CT Kidney Dataset (12,446 images), optimizing for both global fidelity and local precision. Comprehensive experiments demonstrate the model's superiority through quantitative metrics—achieving a 43% improvement in Fréchet Inception Distance (FID = 87 vs. DCGAN's 149, p < 0.01), 16% higher edge sharpness (Sobel gradient magnitude 45.2 ± 3.1 vs. 38.9 ± 4.2), and 0.82 ± 0.05 SSIM scores. Clinical validation by board-certified radiologists confirmed 89% diagnostic plausibility for synthetic images, with particular praise for tumor boundary delineation. The architecture also shows exceptional training stability, reducing loss fluctuations by 34% compared to conventional GANs while efficiently scaling to 128 × 128 resolution. These results establish a new benchmark for privacy-preserving medical data augmentation, offering immediate value for scenarios with limited annotated datasets. Future directions include extension to 3D volumetric synthesis and integration with diffusion models for multi-modal applications, potentially revolutionizing how healthcare institutions generate and share synthetic patient data without compromising privacy.},
  journal     = {Applied AI Letters},
  volume      = {7},
  number      = {1},
  pages       = {e70019},
  year        = {2026},
  doi         = {10.1002/ail2.70019},
  url         = {https://doi.org/10.1002/ail2.70019},
  publisher   = {Wiley},
  google_scholar_id = {-f6ydRqryjwC},
  dimensions        = {true},
}