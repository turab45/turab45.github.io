<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Muhammad Turab </title> <meta name="author" content="Muhammad Turab"> <meta name="description" content="I have selected a few of my research papers, for updated please visit my [Google Scholar](https://scholar.google.com/citations?user=LD8VtagAAAAJ)."> <meta name="keywords" content="turab, muhammad turab, muhammad turab muslim bajeer"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/profile_pic.png?v=9e7fd99c4952cd70c0efdefd84d5033a"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://turab45.github.io/publications/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Muhammad Turab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">I have selected a few of my research papers, for updated please visit my [Google Scholar](https://scholar.google.com/citations?user=LD8VtagAAAAJ).</p> </header> <article> <div class="publications"> <h2 class="bibliography">2026</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff9896"> <div>SIPCOV</div> </abbr> </div> <div id="tremeau2026performingarts" class="col-sm-8"> <div class="title">Movement Analysis in Performing Arts from Human Body Pose Estimation</div> <div class="author"> <a href="https://perso.univ-st-etienne.fr/tremeaua/" rel="external nofollow noopener" target="_blank">Alain Trémeau</a>, Muhammad Turab, <a href="https://laboratoirehubertcurien.univ-st-etienne.fr/en/teams/image-science-computer-vision/staff.html" rel="external nofollow noopener" target="_blank">Damien Muselet</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Philippe Colantoni' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advances on Signal Processing and Computer Vision: Second International Conference, SIPCOV 2025, Silchar, India, August 8–9, 2025, Proceedings</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-032-15809-3_26" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/978-3-032-15809-3_26" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:qUcmZB5y_30C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>In this keynote paper we discuss how to automatically analyze the kinematics of human body movements in the context of dance analysis. We also propose new movement analysis handcrafted features based on Laban Movement Analysis (LMA) and 3D pose estimation of dancers. To evaluate the most significant body parts in kinematics analysis of human body movements, we computed the SHAP value of each body part. The preliminary tests and experiments we carried out using video sequences from AIST++ dataset demonstrate the relevance of these new features based either on short or long periods of time. We also demonstrate that the relevance of the most significant features depends on the specificities of the dance videos dataset to process. To improve the accuracy of handcraft features for dance style classification and their robustness to dataset specificities, we suggest to use domain adaptation techniques, such as feature-based, instance-based or parameter-based techniques.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tremeau2026performingarts</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Movement Analysis in Performing Arts from Human Body Pose Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tr{\'e}meau, Alain and Turab, Muhammad and Muselet, Damien and Colantoni, Philippe}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances on Signal Processing and Computer Vision: Second International Conference, SIPCOV 2025, Silchar, India, August 8--9, 2025, Proceedings}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{331--345}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-032-15809-3_26}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/978-3-032-15809-3_26}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#98df8a"> <div>AIL</div> </abbr> </div> <div id="vavekanand2026edgeawaregan" class="col-sm-8"> <div class="title">A Stable Edge-Aware GAN Approach for Data Augmentation and Privacy-Preserving High-Fidelity CT Synthesis</div> <div class="author"> <a href="https://scholar.google.com/citations?user=LD8VtagAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Raja Vavekanand</a>, Muhammad Turab, and <a href="https://scholar.google.com/citations?user=6IF2rkcAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Teerath Kumar</a> </div> <div class="periodical"> <em>Applied AI Letters</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1002/ail2.70019" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1002/ail2.70019" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:-f6ydRqryjwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Generative adversarial networks (GANs) have shown remarkable potential in medical image synthesis but face persistent challenges in achieving diagnostic-grade quality, particularly in preserving anatomical edges and avoiding mode collapse. To address these limitations, we present a stable edge-aware GAN architecture featuring two key innovations: dynamically learnable bilateral kernels that adaptively enhance structural gradients during training, and a layered generator with interpolated skip connections to maintain spatial coherence. Our methodology leverages adversarial training with Wasserstein regularization on the CT Kidney Dataset (12,446 images), optimizing for both global fidelity and local precision. Comprehensive experiments demonstrate the model’s superiority through quantitative metrics—achieving a 43% improvement in Fréchet Inception Distance (FID = 87 vs. DCGAN’s 149, p &lt; 0.01), 16% higher edge sharpness (Sobel gradient magnitude 45.2 ± 3.1 vs. 38.9 ± 4.2), and 0.82 ± 0.05 SSIM scores. Clinical validation by board-certified radiologists confirmed 89% diagnostic plausibility for synthetic images, with particular praise for tumor boundary delineation. The architecture also shows exceptional training stability, reducing loss fluctuations by 34% compared to conventional GANs while efficiently scaling to 128 × 128 resolution. These results establish a new benchmark for privacy-preserving medical data augmentation, offering immediate value for scenarios with limited annotated datasets. Future directions include extension to 3D volumetric synthesis and integration with diffusion models for multi-modal applications, potentially revolutionizing how healthcare institutions generate and share synthetic patient data without compromising privacy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vavekanand2026edgeawaregan</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Stable Edge-Aware GAN Approach for Data Augmentation and Privacy-Preserving High-Fidelity CT Synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vavekanand, Raja and Turab, Muhammad and Kumar, Teerath}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied AI Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{e70019}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1002/ail2.70019}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1002/ail2.70019}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Wiley}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#8c564b"> <div>arXiv</div> </abbr> </div> <div id="turab2025lowillumination" class="col-sm-8"> <div class="title">A Comprehensive Survey on Image Signal Processing Approaches for Low-Illumination Image Enhancement</div> <div class="author"> Muhammad Turab </div> <div class="periodical"> <em>arXiv preprint arXiv:2502.05995</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2502.05995" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.48550/arXiv.2502.05995" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:qxL8FJ1GzNcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-2-4285F4?logo=googlescholar&amp;labelColor=beige" alt="2 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The usage of digital content (photos and videos) in a variety of applications has increased due to the popularity of multimedia devices. These uses include advertising campaigns, educational resources, and social networking platforms. There is an increasing need for high-quality graphic information as people become more visually focused. However, captured images frequently have poor visibility and a high amount of noise due to the limitations of image-capturing devices and lighting conditions. Improving the visual quality of images taken in low illumination is the aim of low-illumination image enhancement. This problem is addressed by traditional image enhancement techniques, which alter noise, brightness, and contrast. Deep learning-based methods, however, have dominated recently made advances in this area. These methods have effectively reduced noise while preserving important information, showing promising results in the improvement of low-illumination images. An extensive summary of image signal processing methods for enhancing low-illumination images is provided in this paper. Three categories are classified in the review for approaches: hybrid techniques, deep learning-based methods, and traditional approaches. Conventional techniques include denoising, automated white balancing, and noise reduction. Convolutional neural networks (CNNs) are used in deep learningbased techniques to recognize and extract characteristics from low-light images. To get better results, hybrid approaches combine deep learning-based methodologies with more conventional methods. The review also discusses the advantages and limitations of each approach and provides insights into future research directions in this field.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">turab2025lowillumination</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Comprehensive Survey on Image Signal Processing Approaches for Low-Illumination Image Enhancement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Turab, Muhammad}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2502.05995}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2502.05995}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2502.05995}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#7f7f7f"> <div>IPAS</div> </abbr> </div> <div id="jamil2025ishihara" class="col-sm-8"> <div class="title">Optimizing Color Vision Accuracy in Ishihara Tests through Spectral Illumination Optimization</div> <div class="author"> Sonain Jamil and Muhammad Turab </div> <div class="periodical"> <em>In 2025 IEEE 6th International Conference on Image Processing, Applications and Systems (IPAS)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/IPAS63548.2025.10924513" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/IPAS63548.2025.10924513" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:4DMP91E08xMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>In order to design optimal illumination that minimizes or maximizes differences between two sample points, we utilized particle swarm optimization (PSO), a popular algorithm for finding optimal solutions. In this study, we explored the application of PSO to minimize or maximize cost functions such as ∆E, ∆RGB, and Michelson contrast, and to output optimal intensities for designing the emission spectra of the illumination. This approach proved to be more convenient than altering the spectral properties of the object or the observer. The experiment involved capturing sample data from the Ishihara Color Blindness test book using Specim V10E, and measuring the emission spectra of ten LEDs from Ledmotive’s spectrally tunable LED light source using the Hamamatsu PMA-12 spectrometer. Subsequently, we pre-processed the captured data and applied PSO to obtain optimal intensities for the ten LEDs. The resulting optimized intensities were then used to compute the spectrally optimal emission spectrum. To assess the performance of these emission spectra, we illuminated the samples with the optimized spectra using the spectrally tunable LED light source. From the illuminated samples, we concluded that for the minimum cost function, the red number on the sample was not distinguishable from the green background, whereas for the maximum cost function, the red number was distinctive.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jamil2025ishihara</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optimizing Color Vision Accuracy in Ishihara Tests through Spectral Illumination Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jamil, Sonain and Turab, Muhammad}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 IEEE 6th International Conference on Image Processing, Applications and Systems (IPAS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IPAS63548.2025.10924513}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/IPAS63548.2025.10924513}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#bcbd22"> <div>ACIVS</div> </abbr> </div> <div id="turab2026dancestyle" class="col-sm-8"> <div class="title">Dance Style Recognition Using Laban Movement Analysis</div> <div class="author"> Muhammad Turab, <a href="https://www.couleur.org/" rel="external nofollow noopener" target="_blank">Philippe Colantoni</a>, <a href="https://laboratoirehubertcurien.univ-st-etienne.fr/en/teams/image-science-computer-vision/staff.html" rel="external nofollow noopener" target="_blank">Damien Muselet</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alain Trémeau' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advanced Concepts for Intelligent Vision Systems: 22nd International Conference, ACIVS 2025, Tokyo, Japan, July 28–30, 2025, Proceedings</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-032-07343-3_45" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/978-3-032-07343-3_45" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:mVmsd5A6BfQC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The growing interest in automated movement analysis has presented new challenges in recognition of complex human activities including dance. This study focuses on dance style recognition using features extracted using Laban Movement Analysis. Previous studies for dance style recognition often focus on cross-frame movement analysis, which limits the ability to capture temporal context and dynamic transitions between movements. This gap highlights the need for a method that can add temporal context to LMA features. For this, we introduce a novel pipeline which combines 3D pose estimation, 3D human mesh reconstruction, and floor aware body modeling to effectively extract LMA features. To address the temporal limitation, we propose a sliding window approach that captures movement evolution across time in features. These features are then used to train various machine learning methods for classification, and their explainability using explainable AI methods to evaluate the contribution of each feature to classification performance. Our proposed method achieves a highest classification accuracy of 99.18% which shows that the addition of temporal context significantly improves dance style recognition performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">turab2026dancestyle</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dance Style Recognition Using Laban Movement Analysis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Turab, Muhammad and Colantoni, Philippe and Muselet, Damien and Tr{\'e}meau, Alain}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advanced Concepts for Intelligent Vision Systems: 22nd International Conference, ACIVS 2025, Tokyo, Japan, July 28--30, 2025, Proceedings}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{564--575}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-032-07343-3_45}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/978-3-032-07343-3_45}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#17becf"> <div>CAIP</div> </abbr> </div> <div id="turab2025danceemotion" class="col-sm-8"> <div class="title">Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis</div> <div class="author"> Muhammad Turab, <a href="https://www.couleur.org/" rel="external nofollow noopener" target="_blank">Philippe Colantoni</a>, <a href="https://laboratoirehubertcurien.univ-st-etienne.fr/en/teams/image-science-computer-vision/staff.html" rel="external nofollow noopener" target="_blank">Damien Muselet</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alain Trémeau' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Computer Analysis of Images and Patterns: 21st International Conference, CAIP 2025, Las Palmas de Gran Canaria, Spain, September 22–25, 2025, Proceedings</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-032-05060-1_27" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/978-3-032-05060-1_27" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:9ZlFYXVOiuMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper presents a novel framework for emotion recognition in contemporary dance by improving existing Laban Movement Analysis (LMA) feature descriptors and introducing robust, novel descriptors that capture both quantitative and qualitative aspects of the movement. Our approach extracts expressive characteristics from 3D keypoints data of professional dancers performing contemporary dance under various emotional states, and train multiple classifiers, including Random Forests and Support Vector Machines. Additionally, provide in-depth explanation of features and their impact on model predictions using explainable machine learning methods. Overall, our study improves emotion recognition in contemporary dance and offers promising applications in performance analysis, dance training, and human–computer interaction with highest accuracy of 96.85%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">turab2025danceemotion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Turab, Muhammad and Colantoni, Philippe and Muselet, Damien and Tr{\'e}meau, Alain}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Computer Analysis of Images and Patterns: 21st International Conference, CAIP 2025, Las Palmas de Gran Canaria, Spain, September 22--25, 2025, Proceedings}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{317--328}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-032-05060-1_27}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/978-3-032-05060-1_27}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#d62728"> <div>IMTIC</div> </abbr> </div> <div id="irfan2023gotogether" class="col-sm-8"> <div class="title">Go Together: Bridging the Gap between Learners and Teachers</div> <div class="author"> <a href="https://scholar.google.com/citations?user=LD8VtagAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Asim Irfan</a>, <a href="https://scholar.google.com/citations?user=LD8VtagAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Atif Nawaz</a>, Muhammad Turab, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Muhmmad Azeem, Mashal Adnan, Ahsan Mehmood, Sarfaraz Ahmed, Adnan Ashraf' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In 2023 7th International Multi-Topic ICT Conference (IMTIC)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/IMTIC58887.2023.10178623" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:W7OEmFMy1HYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">irfan2023gotogether</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Go Together: Bridging the Gap between Learners and Teachers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Irfan, Asim and Nawaz, Atif and Turab, Muhammad and Azeem, Muhmmad and Adnan, Mashal and Mehmood, Ahsan and Ahmed, Sarfaraz and Ashraf, Adnan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 7th International Multi-Topic ICT Conference (IMTIC)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--7}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IMTIC58887.2023.10178623}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/IMTIC58887.2023.10178623}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#9467bd"> <div>BioMedInf</div> </abbr> </div> <div id="turab2023digitaltwinsmetaverse" class="col-sm-8"> <div class="title">A Comprehensive Survey of Digital Twins in Healthcare in the Era of Metaverse</div> <div class="author"> Muhammad Turab and Sonain Jamil </div> <div class="periodical"> <em>BioMedInformatics</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3390/biomedinformatics3030039" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.3390/biomedinformatics3030039" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:YsMSGLbcyi4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-100-4285F4?logo=googlescholar&amp;labelColor=beige" alt="100 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Digital twins (DTs) are becoming increasingly popular in various industries, and their potential for healthcare in the metaverse continues to attract attention. The metaverse is a virtual world where individuals interact with digital replicas of themselves and the environment. This paper focuses on personalized and precise medicine and examines the current application of DTs in healthcare within the metaverse. Healthcare practitioners may use immersive virtual worlds to replicate medical scenarios, improve teaching experiences, and provide personalized care to patients. However, the integration of DTs in the metaverse poses technical, regulatory, and ethical challenges that need to be addressed, including data privacy, standards, and accessibility. Through this examination, we aim to provide insights into the transformative potential of DTs in healthcare within the metaverse and encourage further research and development in this exciting domain.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">turab2023digitaltwinsmetaverse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Comprehensive Survey of Digital Twins in Healthcare in the Era of Metaverse}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Turab, Muhammad and Jamil, Sonain}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{BioMedInformatics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{563--584}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/biomedinformatics3030039}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.3390/biomedinformatics3030039}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MDPI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#8c564b"> <div>arXiv</div> </abbr> </div> <div id="kumar2023audrandaug" class="col-sm-8"> <div class="title">AudRandAug: Random Image Augmentations for Audio Classification</div> <div class="author"> <a href="https://scholar.google.com/citations?user=6IF2rkcAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Teerath Kumar</a>, Muhammad Turab, <a href="https://scholar.google.com/citations?user=ZkQuEgMAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alessandra Mileo</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Malika Bendechache, Takfarinas Saber' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2309.04762</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2309.04762" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.48550/arXiv.2309.04762" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:WF5omc3nYNoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-14-4285F4?logo=googlescholar&amp;labelColor=beige" alt="14 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Data augmentation has proven to be effective in training neural networks. Recently, a method called RandAug was proposed, randomly selecting data augmentation techniques from a predefined search space. RandAug has demonstrated significant performance improvements for image-related tasks while imposing minimal computational overhead. However, no prior research has explored the application of RandAug specifically for audio data augmentation, which converts audio into an image-like pattern. To address this gap, we introduce AudRandAug, an adaptation of RandAug for audio data. AudRandAug selects data augmentation policies from a dedicated audio search space. To evaluate the effectiveness of AudRandAug, we conducted experiments using various models and datasets. Our findings indicate that AudRandAug outperforms other existing data augmentation methods regarding accuracy performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kumar2023audrandaug</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AudRandAug: Random Image Augmentations for Audio Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kumar, Teerath and Turab, Muhammad and Mileo, Alessandra and Bendechache, Malika and Saber, Takfarinas}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2309.04762}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2309.04762}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2309.04762}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e377c2"> <div>MURJET</div> </abbr> </div> <div id="memon2023alzheimerensemble" class="col-sm-8"> <div class="title">An Ensemble of CNN Architectures for Early Detection of Alzheimer’s Disease Using Brain MRI</div> <div class="author"> Zainab Memon, Muhammad Turab, <a href="https://scholar.google.com/citations?user=LD8VtagAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sanam Narejo</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Muhammad Tahir Korejo' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Mehran University Research Journal of Engineering &amp; Technology</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.22581/muet1982.2304.2767" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.22581/muet1982.2304.2767" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-5-4285F4?logo=googlescholar&amp;labelColor=beige" alt="5 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Early detection of Alzheimer’s disease (AD) has proven to be helpful and effective in preventing the disease. If the risks and symptoms of AD are detected earlier, then it seems rather promising that the death ratio of AD might decrease as it can help a lot of patients get treated before it’s too late. Our study demonstrates promising results, achieving a remarkable accuracy of 96.52% through the utilization of the EfficientNetB2 and EfficientNetB3 models. By leveraging transfer learning, we leverage pre-trained models’ knowledge to optimize the learning process, while ensemble learning further improves performance by aggregating predictions from multiple models. The integration of these methodologies provides an effective and efficient means of detecting Alzheimer’s Disease at an early stage, thereby offering potential benefits to patients, caregivers, and healthcare providers alike. These findings pave the way for improved diagnostic tools and contribute to the advancement of AD research and patient care.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">memon2023alzheimerensemble</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An Ensemble of CNN Architectures for Early Detection of Alzheimer's Disease Using Brain MRI}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Memon, Zainab and Turab, Muhammad and Narejo, Sanam and Korejo, Muhammad Tahir}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Mehran University Research Journal of Engineering \&amp; Technology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{42}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{140--147}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.22581/muet1982.2304.2767}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.22581/muet1982.2304.2767}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#8c564b"> <div>arXiv</div> </abbr> </div> <div id="kumar2023imagedataaugmentation" class="col-sm-8"> <div class="title">Image Data Augmentation Approaches: A Comprehensive Survey and Future directions</div> <div class="author"> <a href="https://scholar.google.com/citations?user=6IF2rkcAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Teerath Kumar</a>, <a href="https://scholar.google.com/citations?user=ZkQuEgMAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alessandra Mileo</a>, <a href="https://scholar.google.com/citations?user=LD8VtagAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Rob Brennan</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Malika Bendechache' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2301.02830</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.48550/arXiv.2301.02830" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.48550/arXiv.2301.02830" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:Wp0gIr-vW9MC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-43-4285F4?logo=googlescholar&amp;labelColor=beige" alt="43 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kumar2023imagedataaugmentation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Image Data Augmentation Approaches: A Comprehensive Survey and Future directions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kumar, Teerath and Mileo, Alessandra and Brennan, Rob and Bendechache, Malika}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2301.02830}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2301.02830}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2301.02830}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#1f77b4"> <div>IJAIA</div> </abbr> </div> <div id="kumar2022forgedcharacterdatasets" class="col-sm-8"> <div class="title">Forged Character Detection Datasets: Passports, Driving Licences and Visa Stickers</div> <div class="author"> <a href="https://scholar.google.com/citations?user=6IF2rkcAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Teerath Kumar</a>, Muhammad Turab, <a href="https://scholar.google.com/citations?user=LD8VtagAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Shahnawaz Talpur</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Rob Brennan, Malika Bendechache' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>International Journal of Artificial Intelligence &amp; Applications</em>, Mar 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.5121/ijaia.2022.13202" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.5121/ijaia.2022.13202" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:u5HHmVD_uO8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-22-4285F4?logo=googlescholar&amp;labelColor=beige" alt="22 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Forged documents specifically passport, driving licence and VISA stickers are used for fraud purposes including robbery, theft and many more. So detecting forged characters from documents is a significantly important and challenging task in digital forensic imaging. Forged characters detection has two big challenges. First challenge is, data for forged characters detection is extremely difficult to get due to several reasons including limited access of data, unlabeled data or work is done on private data. Second challenge is, deep learning (DL) algorithms require labeled data, which poses a further challenge as getting labeled is tedious, time-consuming, expensive and requires domain expertise. To end these issues, in this paper we propose a novel algorithm, which generates the three datasets namely forged characters detection for passport (FCD-P), forged characters detection for driving licence (FCD-D) and forged characters detection for VISA stickers (FCD-V). To the best of our knowledge, we are the first to release these datasets. The proposed algorithm starts by reading plain document images, simulates forging simulation tasks on five different countries’ passports, driving licences and VISA stickers. Then it keeps the bounding boxes as a track of the forged characters as a labeling process. Furthermore, considering the real world scenario, we performed the selected data augmentation accordingly. Regarding the stats of datasets, each dataset consists of 15000 images having size of 950 x 550 of each. For further research purpose we release our algorithm code 1 and, datasets ie FCD-P 2, FCD-D 3 and FCD-V 4.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kumar2022forgedcharacterdatasets</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Forged Character Detection Datasets: Passports, Driving Licences and Visa Stickers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kumar, Teerath and Turab, Muhammad and Talpur, Shahnawaz and Brennan, Rob and Bendechache, Malika}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Artificial Intelligence \&amp; Applications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{21--35}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5121/ijaia.2022.13202}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.5121/ijaia.2022.13202}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#1f77b4"> <div>IJAIA</div> </abbr> </div> <div id="turab2022multifeatureaudio" class="col-sm-8"> <div class="title">Investigating Multi-Feature Selection and Ensembling for Audio Classification</div> <div class="author"> Muhammad Turab, <a href="https://scholar.google.com/citations?user=6IF2rkcAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Teerath Kumar</a>, <a href="https://malikabendechache.github.io/" rel="external nofollow noopener" target="_blank">Malika Bendechache</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Takfarinas Saber' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>International Journal of Artificial Intelligence &amp; Applications</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.5121/ijaia.2022.13306" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.5121/ijaia.2022.13306" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:d1gkVwhDpl0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-63-4285F4?logo=googlescholar&amp;labelColor=beige" alt="63 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Deep Learning (DL) algorithms have shown impressive performance in diverse domains. Among them, audio has attracted many researchers over the last couple of decades due to some interesting patterns–particularly in classification of audio data. For better performance of audio classification, feature selection and combination play a key role as they have the potential to make or break the performance of any DL model. To investigate this role, we conduct an extensive evaluation of the performance of several cutting-edge DL models (i.e., Convolutional Neural Network, EfficientNet, MobileNet, Supper Vector Machine and Multi-Perceptron) with various state-of-the-art audio features (i.e., Mel Spectrogram, Mel Frequency Cepstral Coefficients, and Zero Crossing Rate) either independently or as a combination (i.e., through ensembling) on three different datasets (i.e., Free Spoken Digits Dataset, Audio Urdu Digits Dataset, and Audio Gujarati Digits Dataset). Overall, results suggest feature selection depends on both the dataset and the model. However, feature combinations should be restricted to the only features that already achieve good performances when used individually (i.e., mostly Mel Spectrogram, Mel Frequency Cepstral Coefficients). Such feature combination/ensembling enabled us to outperform the previous state-of-the-art results irrespective of our choice of DL model.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">turab2022multifeatureaudio</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Investigating Multi-Feature Selection and Ensembling for Audio Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Turab, Muhammad and Kumar, Teerath and Bendechache, Malika and Saber, Takfarinas}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Artificial Intelligence \&amp; Applications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{69--84}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5121/ijaia.2022.13306}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.5121/ijaia.2022.13306}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff7f0e"> <div>AISCA</div> </abbr> </div> <div id="kumar2022detectiondatasets" class="col-sm-8"> <div class="title">Detection Datasets: Forged Characters for Passport and Driving Licence</div> <div class="author"> <a href="https://scholar.google.com/citations?user=6IF2rkcAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Teerath Kumar</a>, Muhammad Turab, <a href="https://scholar.google.com/citations?user=LD8VtagAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Shahnawaz Talpur</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Rob Brennan, Malika Bendechache' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 6th International Conference on Artificial Intelligence, Soft Computing and Applications (AISCA 2022)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.5121/csit.2022.120204" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.5121/csit.2022.120204" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Forged characters detection from personal documents including a passport or a driving licence is an extremely important and challenging task in digital image forensics, as forged information on personal documents can be used for fraud purposes including theft, robbery etc. For any detection task ie forged character detection, deep learning models are data hungry and getting the forged characters dataset for personal documents is very difficult due to various reasons, including information privacy, unlabeled data or existing work is evaluated on private datasets with limited access and getting data labelled is another big challenge. To address these issues, we propose a new algorithm that generates two new datasets named forged characters detection on passport (FCD-P) and forged characters detection on driving licence (FCD-D). To the best of our knowledge, we are the first to release these datasets. The proposed algorithm first reads the plain image, then performs forging tasks ie randomly changes the position of the random character or randomly adds little noise. At the same time, the algorithm also records the bounding boxes of the forged characters. To meet real world situations, we perform multiple data augmentation on cards very carefully. Overall, each dataset consists of 15000 images, each image with size of 950 x 550. Our algorithm code, FCD-P and FCD-D are publicly available.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kumar2022detectiondatasets</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Detection Datasets: Forged Characters for Passport and Driving Licence}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kumar, Teerath and Turab, Muhammad and Talpur, Shahnawaz and Brennan, Rob and Bendechache, Malika}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{6th International Conference on Artificial Intelligence, Soft Computing and Applications (AISCA 2022)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{45--54}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5121/csit.2022.120204}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.5121/csit.2022.120204}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2ca02c"> <div>ICETECC</div> </abbr> </div> <div id="khan2022ddr" class="col-sm-8"> <div class="title">Data Dimension Reduction Makes ML Algorithms Efficient</div> <div class="author"> <a href="https://scholar.google.com/citations?user=LD8VtagAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Wisal Khan</a>, Muhammad Turab, Waqas Ahmad, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Syed Hasnat Ahmad, Kelash Kumar, Bin Luo' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In 2022 International Conference on Emerging Technologies in Electronics, Computing and Communication (ICETECC)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICETECC56662.2022.10069527" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICETECC56662.2022.10069527" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:qjMakFHDy7sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-13-4285F4?logo=googlescholar&amp;labelColor=beige" alt="13 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Data dimension reduction (DDR) is all about mapping data from high dimensions to low dimensions, various techniques of DDR are being used for image dimension reduction like Random Projections, Principal Component Analysis (PCA), the Variance approach, LSA-Transform, the Combined and Direct approaches, and the New Random Approach. Auto-encoders (AE) are used to learn end-to-end mapping. In this paper, we demonstrate that pre-processing not only speeds up the algorithms but also improves accuracy in both supervised and unsupervised learning. In pre-processing of DDR, first PCA based DDR is used for supervised learning, then we explore AE based DDR for unsupervised learning. In PCA based DDR, we first compare supervised learning algorithms accuracy and time before and after applying PCA. Similarly, in AE based DDR, we compare unsupervised learning algorithm accuracy and time before and after AE representation learning. Supervised learning algorithms including support-vector machines (SVM), Decision Tree with GINI index, Decision Tree with entropy and Stochastic Gradient Descent classifier (SGDC) and unsupervised learning algorithm including K-means clustering, are used for classification purpose. We used two datasets MNIST and FashionMNIST Our experiment shows that there is massive improvement in accuracy and time reduction after pre-processing in both supervised and unsupervised learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">khan2022ddr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data Dimension Reduction Makes ML Algorithms Efficient}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khan, Wisal and Turab, Muhammad and Ahmad, Waqas and Ahmad, Syed Hasnat and Kumar, Kelash and Luo, Bin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 International Conference on Emerging Technologies in Electronics, Computing and Communication (ICETECC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICETECC56662.2022.10069527}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/ICETECC56662.2022.10069527}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2ca02c"> <div>ICETECC</div> </abbr> </div> <div id="sarwar2022audioaid" class="col-sm-8"> <div class="title">Advanced Audio Aid for Blind People</div> <div class="author"> Savera Sarwar, Muhammad Turab, Danish Channa, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Aisha Chandio, M. Uzair Sohu, Vikram Kumar' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In 2022 International Conference on Emerging Technologies in Electronics, Computing and Communication (ICETECC)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICETECC56662.2022.10069052" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICETECC56662.2022.10069052" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LD8VtagAAAAJ&amp;citation_for_view=LD8VtagAAAAJ:UeHWp8X0CEIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-30-4285F4?logo=googlescholar&amp;labelColor=beige" alt="30 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>One of the most important senses in human life is vision, without it one’s life is totally filled with darkness. According to WHO globally millions of people are visually impaired estimated there are 285 million, of whom some millions are blind. Unfortunately, there are around 2.4 million people are blind in our beloved country Pakistan. Human are a crucial part of society and the blind community is a main part of society. The technologies are grown so far to make the life of humans easier more comfortable and more reliable for. However, this disability of the blind community would reduce their chance of using such innovative products. Therefore, the visually impaired community believe that they are burden to other societies and they do not capture in normal activities separates the blind people from society and because of this believe did not participate in the normally tasks of society. The visual impair people mainly face most of the problems in this real-time The aim of this work is to turn the real time world into an audio world by telling blind person about the objects in their way and can read printed text. This will enable blind persons to identify the things and read the text without any external help just by using the object detection and reading system in real time. Objective of this work: i) Object detection ii) Read printed text, using state-of-the-art (SOTA) technology.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sarwar2022audioaid</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Advanced Audio Aid for Blind People}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sarwar, Savera and Turab, Muhammad and Channa, Danish and Chandio, Aisha and Sohu, M. Uzair and Kumar, Vikram}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 International Conference on Emerging Technologies in Electronics, Computing and Communication (ICETECC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICETECC56662.2022.10069052}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/ICETECC56662.2022.10069052}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Muhammad Turab. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>